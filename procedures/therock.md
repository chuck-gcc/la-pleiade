üöÄ Fiche compl√®te ‚Äî Installation et tests de TheRock + ROCm
üß≠ Contexte

Tu travailles dans une arborescence structur√©e :

pleiades/
‚îú‚îÄ‚îÄ atlas/      ‚Üê build et SDK ROCm via TheRock
‚îú‚îÄ‚îÄ merope/     ‚Üê environnement Python / PyTorch ROCm
‚îú‚îÄ‚îÄ electra/    ‚Üê environnements CUDA/NVIDIA
‚îú‚îÄ‚îÄ alcyone/    ‚Üê outils CPU / debug
‚îî‚îÄ‚îÄ docker/     ‚Üê images Docker CPU/ROCm/CUDA


Ton GPU principal :
AMD Radeon PRO W7900 Dual Slot (gfx1100)
Architecture RDNA3 compatible ROCm 7.x.

üß± 1Ô∏è‚É£ Installation de base : ROCm Core SDK avec TheRock
üì¶ Pr√©requis syst√®me

Dans ton container ou ta machine :

apt update
apt install -y automake cmake g++ gfortran git git-lfs \
  libegl1-mesa-dev libtool ninja-build patchelf pip \
  pkg-config python3-dev python3-venv xxd

üß∞ Pr√©parer un environnement Python isol√©
cd pleiades/atlas
python3 -m venv .rockenv
source .rockenv/bin/activate

ü™® Cloner et configurer TheRock
git clone https://github.com/ROCm/TheRock.git
cd TheRock
pip install -r requirements.txt

üì• T√©l√©charger les sources ROCm (librairies et outils)
python build_tools/fetch_sources.py


‚è≥ T√©l√©charge ~16‚Äì17 Go (rocBLAS, MIOpen, HIP, rocRAND, etc.)

‚öôÔ∏è Compiler avec CMake + Ninja

D√©finis ta cible GPU (ex. gfx1100 pour W7900) :

cmake -B build -GNinja . \
  -DTHEROCK_AMDGPU_TARGETS=gfx1100 \
  -DTHEROCK_ENABLE_ALL=OFF \
  -DTHEROCK_ENABLE_HIP_RUNTIME=ON \
  -DTHEROCK_ENABLE_BLAS=ON \
  -DTHEROCK_ENABLE_ML_LIBS=ON


Lance la compilation :

cmake --build build

üß™ V√©rification de la build

Test basique :

ctest --test-dir build


Attendu :

100% tests passed, 0 tests failed out of 24

üìÇ Structure de sortie

Une fois le build termin√©, les binaires sont ici :

pleiades/atlas/build/dist/rocm/
‚îú‚îÄ‚îÄ bin/          ‚Üí rocm-smi, rocminfo, hipcc, etc.
‚îú‚îÄ‚îÄ lib/          ‚Üí libamdhip64.so, librocblas.so, libMIOpen.so...
‚îú‚îÄ‚îÄ include/      ‚Üí headers HIP et ROCm
‚îî‚îÄ‚îÄ share/        ‚Üí scripts et doc

üß© 2Ô∏è‚É£ Test du runtime ROCm (niveau syst√®me)

V√©rifie la d√©tection du GPU :

build/dist/rocm/bin/rocminfo | grep Name


Attendu :

Name: gfx1100


V√©rifie la gestion de la carte :

build/dist/rocm/bin/rocm-smi


Affiche temp√©rature, charge, m√©moire, puissance, etc.

üß† 3Ô∏è‚É£ Int√©gration √† ton environnement Python (Merope)
üß∞ Cr√©er ton venv Python
cd pleiades/merope
python3 -m venv venv
source venv/bin/activate

üîó Lier Merope ‚Üî Atlas (acc√®s au ROCm build local)

Cr√©e scripts/activate_rocm.sh :

#!/bin/bash
ATLAS_ROOT="$(realpath ../atlas)"
ROCM_BIN="$ATLAS_ROOT/build/dist/rocm/bin"
ROCM_LIB="$ATLAS_ROOT/build/dist/rocm/lib"

export PATH="$ROCM_BIN:$PATH"
export LD_LIBRARY_PATH="$ROCM_LIB:$LD_LIBRARY_PATH"

echo "[Merope] Connected to ROCm from Atlas."


Et rends-le ex√©cutable :

chmod +x scripts/activate_rocm.sh


√Ä chaque session :

source scripts/activate_rocm.sh

üî• 4Ô∏è‚É£ Installation de PyTorch ROCm

D√©sinstalle les versions CUDA :

pip uninstall torch torchvision torchaudio -y


Puis installe la version ROCm officielle :

pip install --pre torch torchvision torchaudio \
  --index-url https://download.pytorch.org/whl/nightly/rocm7.0


V√©rifie :

python -c "import torch; print(torch.__version__, torch.version.hip)"


R√©sultat attendu :

2.9.0+rocm7.0 7.0.2

üßÆ 5Ô∏è‚É£ Test de calcul GPU : test_gpu.py
import torch
print("PyTorch:", torch.__version__)
print("HIP:", torch.version.hip)
print("GPU:", torch.cuda.get_device_name(0))
a = torch.randn((4096, 4096), device="cuda")
b = torch.randn((4096, 4096), device="cuda")
c = torch.matmul(a, b)
print("Done:", c.sum().item())


‚úÖ R√©sultat attendu :

GPU: AMD Radeon PRO W7900 Dual Slot
Done: <valeur>

üìä 6Ô∏è‚É£ Test de performance et logs : test_gpu_verbose.py

Script de stress avec logs GPU (rocm-smi) :

Alloue des matrices (16k √ó 16k ou 32k √ó 32k),

Effectue un matmul sur GPU,

Affiche temp√©rature, puissance, m√©moire,

Mesure le temps exact du kernel.

python test_gpu_verbose.py


Sortie attendue :

GPU use (%) : 99
Power (W): 500+
Matrix multiplication done on GPU!

üßæ 7Ô∏è‚É£ V√©rifications suppl√©mentaires
Test	Commande	R√©sultat attendu
D√©tection GPU	`rocminfo	grep Name`
Charge GPU	rocm-smi --showuse	GPU use (%) > 90
Libs ROCm li√©es √† PyTorch	`ldd $(python -c "import torch; import os; print(os.path.join(os.path.dirname(torch.file), 'lib/libtorch_hip.so'))")	grep rocm`
Variable HIP	echo $HIP_VISIBLE_DEVICES	Doit √™tre 0 ou vide
Backend PyTorch	torch.version.hip	Non None
‚ö° 8Ô∏è‚É£ Optimisation (pour saturer le GPU)

Pour forcer une charge √† 99 % :

N = 32768
for i in range(5):
    a = torch.randn((N, N), dtype=torch.float16, device="cuda")
    b = torch.randn((N, N), dtype=torch.float16, device="cuda")
    c = torch.matmul(a, b)
    torch.cuda.synchronize()


Surveille en parall√®le :

watch -n 0.5 rocm-smi --showuse --showpower --showtemp

‚úÖ 9Ô∏è‚É£ R√©sum√© des points de validation
√âtape	Objectif	Outil / commande	R√©sultat attendu
Build TheRock	Compiler ROCm SDK local	cmake --build build	dist/rocm cr√©√©
ROCm visible	GPU d√©tect√©	rocminfo	gfx1100
Environnement Merope	Activation du PATH ROCm	source scripts/activate_rocm.sh	Chemins export√©s
PyTorch ROCm	Framework pr√™t	torch.version.hip	7.x
Matmul test	Calcul GPU	python test_gpu.py	R√©sultat num√©rique
GPU load	V√©rifier activit√©	rocm-smi	use (%) ~99
üì¶ 10Ô∏è‚É£ Nettoyage / Maintenance

Pour reconstruire ROCm :

cd pleiades/atlas/TheRock
rm -rf build
cmake -B build -GNinja .
cmake --build build


Pour mettre √† jour les sources ROCm :

python build_tools/fetch_sources.py --update

üß† En r√©sum√©
Composant	R√¥le
TheRock	Syst√®me de build CMake/Ninja unifi√© pour ROCm
ROCm SDK (Atlas)	Librairies HIP, rocBLAS, MIOpen, etc.
Merope	Environnement Python/PyTorch ROCm
activate_rocm.sh	Pont entre Merope et Atlas
PyTorch ROCm wheel	Backend HIP pour Python
rocm-smi / rocminfo	Outils de diagnostic GPU
test_gpu_verbose.py	Benchmark et monitoring GPU
